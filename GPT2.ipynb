{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "df = pd.read_csv('all_data.csv')\n",
    "text = df['Text'].to_list()\n",
    "label = df['Lang'].to_list()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "langs = glob.glob(\"data/*.csv\")\n",
    "langs = [lang.split(\"\\\\\")[1].split(\".\")[0] for lang in langs]\n",
    "langs = [(lang, i) for i, lang in enumerate(langs)]\n",
    "dict(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import (set_seed,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "# Initialisation des param√®tres\n",
    "\n",
    "set_seed(42)\n",
    "epochs = 4 # Entre 2 et 4\n",
    "batch_size = 32\n",
    "max_length = 60\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'openai-community/gpt2' # 'gpt2-medium', 'gpt2-large'\n",
    "labels_ids = {'ARA': 0,\n",
    "                'CHI': 1,\n",
    "                'FRE': 2,\n",
    "                'GER': 3,\n",
    "                'HIN': 4,\n",
    "                'ITA': 5,\n",
    "                'JPN': 6,\n",
    "                'KOR': 7,\n",
    "                'SPA': 8,\n",
    "                'TEL': 9,\n",
    "                'TUR': 10}\n",
    "n_labels = len(labels_ids)\n",
    "class GPT2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File {path} not found\")\n",
    "        df = pd.read_csv(path)\n",
    "        self.texts = df['Text'].to_list()\n",
    "        self.labels = df['Lang'].to_list()    \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text':self.texts[item],\n",
    "            'label':self.labels[item]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class GPT2Collator:\n",
    "    def __init__(self, tokenizer, labels_encoder, max_length=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = tokenizer.model_max_length if max_length is None else max_length\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        texts = [sample['text'] for sample in samples]\n",
    "        labels = [sample['label'] for sample in samples]\n",
    "        \n",
    "        # Tokenization\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "        inputs['labels'] = torch.tensor([self.labels_encoder[label] for label in labels])\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "def train(dataloader, optimizer, scheduler, device_):\n",
    "    global model\n",
    "    \n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        \n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "    \n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "        \n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple along with the logits. We will use logits\n",
    "        # later to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Convert these logits to list of predicted labels values.\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Return all true labels and prediction for future evaluations.\n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "    # Use global variable for model.\n",
    "    global model\n",
    "\n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    #total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in dataloader:\n",
    "\n",
    "        # add original labels\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple along with the logits. We will use logits\n",
    "            # later to to calculate training accuracy.\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # get predicitons to list\n",
    "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "            # update list\n",
    "            predictions_labels += predict_content\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Return all true labels and prediciton for future evaluations.\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path = model_name, num_labels = n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = GPT2Collator(tokenizer=tokenizer, \n",
    "                                            labels_encoder=labels_ids, \n",
    "                                            max_length=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = GPT2Dataset(path='train.csv')\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  GPT2Dataset(path='test.csv')\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    # Perform one full pass over the training set.\n",
    "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "    # Get prediction form model on validation data. \n",
    "    print('Validation on batches...')\n",
    "    valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "    val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "    # Print loss and accuracy values to see how training evolves.\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "    print()\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    all_loss['val_loss'].append(val_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves.\n",
    "plt.plot(all_loss[\"train_loss\"], all_loss[\"val_loss\"])\n",
    "# Plot accuracy curves.\n",
    "plt.plot(all_acc[\"train_acc\"], all_acc[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get prediction form model on validation data. This is where you should use\n",
    "# your test data.\n",
    "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
    "\n",
    "# Create the evaluation report.\n",
    "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
    "# Show the evaluation report.\n",
    "print(evaluation_report)\n",
    "\n",
    "# Plot confusion matrix.\n",
    "plt.colorbar(y_true=true_labels, y_pred=predictions_labels, \n",
    "                      classes=list(labels_ids.keys()), normalize=True, \n",
    "                      magnify=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
