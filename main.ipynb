{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 54953353516333b18a5c240985cdb19ed8b98b05
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[\"Lang\"].unique():\n",
    "    df[df[\"Lang\"] == i].to_csv(f\"data/{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "lang = GER , nb= 900 , moy length= 1953.183 , max length= 4141 , min length= 333 , esperance length= 367.825 , moy nb mot : 376.533 , max nb mot: 806 , min nb mot: 63 , esperance nb mot: 69.772 , moy nb phrase: 18.434 , max nb phrase: 40 , min nb phrase: 5 , esperance nb phrase: 4.336\n",
      "lang = TUR , nb= 900 , moy length= 1842.067 , max length= 4154 , min length= 68 , esperance length= 437.446 , moy nb mot : 352.671 , max nb mot: 776 , min nb mot: 15 , esperance nb mot: 83.303 , moy nb phrase: 18.611 , max nb phrase: 44 , min nb phrase: 1 , esperance nb phrase: 5.567\n",
      "lang = CHI , nb= 900 , moy length= 1852.431 , max length= 4620 , min length= 91 , esperance length= 432.747 , moy nb mot : 361.812 , max nb mot: 876 , min nb mot: 16 , esperance nb mot: 82.095 , moy nb phrase: 18.448 , max nb phrase: 38 , min nb phrase: 2 , esperance nb phrase: 5.329\n",
      "lang = TEL , nb= 900 , moy length= 1900.249 , max length= 4073 , min length= 747 , esperance length= 386.31 , moy nb mot : 359.039 , max nb mot: 785 , min nb mot: 136 , esperance nb mot: 72.375 , moy nb phrase: 17.713 , max nb phrase: 46 , min nb phrase: 1 , esperance nb phrase: 5.708\n",
      "lang = ARA , nb= 900 , moy length= 1579.736 , max length= 3654 , min length= 62 , esperance length= 472.772 , moy nb mot : 310.129 , max nb mot: 715 , min nb mot: 11 , esperance nb mot: 90.278 , moy nb phrase: 13.026 , max nb phrase: 34 , min nb phrase: 1 , esperance nb phrase: 5.715\n",
      "lang = SPA , nb= 900 , moy length= 1841.82 , max length= 3854 , min length= 400 , esperance length= 384.878 , moy nb mot : 361.957 , max nb mot: 774 , min nb mot: 73 , esperance nb mot: 75.019 , moy nb phrase: 14.81 , max nb phrase: 36 , min nb phrase: 1 , esperance nb phrase: 5.077\n",
      "lang = HIN , nb= 900 , moy length= 2038.392 , max length= 4378 , min length= 826 , esperance length= 428.456 , moy nb mot : 385.337 , max nb mot: 804 , min nb mot: 168 , esperance nb mot: 79.476 , moy nb phrase: 18.594 , max nb phrase: 45 , min nb phrase: 1 , esperance nb phrase: 5.396\n",
      "lang = JPN , nb= 900 , moy length= 1594.398 , max length= 3352 , min length= 59 , esperance length= 467.699 , moy nb mot : 312.776 , max nb mot: 669 , min nb mot: 12 , esperance nb mot: 90.176 , moy nb phrase: 18.074 , max nb phrase: 38 , min nb phrase: 3 , esperance nb phrase: 5.248\n",
      "lang = KOR , nb= 900 , moy length= 1738.837 , max length= 3592 , min length= 96 , esperance length= 487.093 , moy nb mot : 336.628 , max nb mot: 696 , min nb mot: 22 , esperance nb mot: 93.881 , moy nb phrase: 19.229 , max nb phrase: 43 , min nb phrase: 2 , esperance nb phrase: 5.795\n",
      "lang = FRE , nb= 900 , moy length= 1809.983 , max length= 3197 , min length= 327 , esperance length= 383.016 , moy nb mot : 353.77 , max nb mot: 628 , min nb mot: 60 , esperance nb mot: 74.776 , moy nb phrase: 17.006 , max nb phrase: 64 , min nb phrase: 1 , esperance nb phrase: 5.124\n",
      "lang = ITA , nb= 900 , moy length= 1665.043 , max length= 3147 , min length= 82 , esperance length= 397.033 , moy nb mot : 325.79 , max nb mot: 613 , min nb mot: 15 , esperance nb mot: 76.374 , moy nb phrase: 13.711 , max nb phrase: 35 , min nb phrase: 2 , esperance nb phrase: 4.546\n"
=======
      "lang = GER , nb= 900 , moy length= 1952.983 , max length= 4141 , min length= 333 , esperance length= 367.666 , moy nb mot : 376.533 , max nb mot: 806 , min nb mot: 63 , esperance nb mot: 69.772 , moy nb phrase: 18.434 , max nb phrase: 40 , min nb phrase: 5 , esperance nb phrase: 4.336\n",
      "lang = TUR , nb= 900 , moy length= 1841.998 , max length= 4154 , min length= 68 , esperance length= 437.412 , moy nb mot : 352.671 , max nb mot: 776 , min nb mot: 15 , esperance nb mot: 83.303 , moy nb phrase: 18.611 , max nb phrase: 44 , min nb phrase: 1 , esperance nb phrase: 5.567\n",
      "lang = CHI , nb= 900 , moy length= 1852.389 , max length= 4620 , min length= 91 , esperance length= 432.715 , moy nb mot : 361.812 , max nb mot: 876 , min nb mot: 16 , esperance nb mot: 82.095 , moy nb phrase: 18.448 , max nb phrase: 38 , min nb phrase: 2 , esperance nb phrase: 5.329\n",
      "lang = TEL , nb= 900 , moy length= 1900.13 , max length= 4073 , min length= 747 , esperance length= 386.245 , moy nb mot : 359.039 , max nb mot: 785 , min nb mot: 136 , esperance nb mot: 72.375 , moy nb phrase: 17.713 , max nb phrase: 46 , min nb phrase: 1 , esperance nb phrase: 5.708\n",
      "lang = ARA , nb= 900 , moy length= 1579.616 , max length= 3653 , min length= 62 , esperance length= 472.709 , moy nb mot : 310.129 , max nb mot: 715 , min nb mot: 11 , esperance nb mot: 90.278 , moy nb phrase: 13.026 , max nb phrase: 34 , min nb phrase: 1 , esperance nb phrase: 5.715\n",
      "lang = SPA , nb= 900 , moy length= 1841.582 , max length= 3853 , min length= 400 , esperance length= 384.748 , moy nb mot : 361.957 , max nb mot: 774 , min nb mot: 73 , esperance nb mot: 75.019 , moy nb phrase: 14.81 , max nb phrase: 36 , min nb phrase: 1 , esperance nb phrase: 5.077\n",
      "lang = HIN , nb= 900 , moy length= 2038.253 , max length= 4376 , min length= 826 , esperance length= 428.374 , moy nb mot : 385.337 , max nb mot: 804 , min nb mot: 168 , esperance nb mot: 79.476 , moy nb phrase: 18.594 , max nb phrase: 45 , min nb phrase: 1 , esperance nb phrase: 5.396\n",
      "lang = JPN , nb= 900 , moy length= 1594.353 , max length= 3351 , min length= 59 , esperance length= 467.684 , moy nb mot : 312.776 , max nb mot: 669 , min nb mot: 12 , esperance nb mot: 90.176 , moy nb phrase: 18.074 , max nb phrase: 38 , min nb phrase: 3 , esperance nb phrase: 5.248\n",
      "lang = KOR , nb= 900 , moy length= 1738.778 , max length= 3592 , min length= 96 , esperance length= 487.083 , moy nb mot : 336.628 , max nb mot: 696 , min nb mot: 22 , esperance nb mot: 93.881 , moy nb phrase: 19.229 , max nb phrase: 43 , min nb phrase: 2 , esperance nb phrase: 5.795\n",
      "lang = FRE , nb= 900 , moy length= 1809.782 , max length= 3194 , min length= 327 , esperance length= 382.906 , moy nb mot : 353.77 , max nb mot: 628 , min nb mot: 60 , esperance nb mot: 74.776 , moy nb phrase: 17.006 , max nb phrase: 64 , min nb phrase: 1 , esperance nb phrase: 5.124\n",
      "lang = ITA , nb= 900 , moy length= 1664.73 , max length= 3147 , min length= 82 , esperance length= 396.913 , moy nb mot : 325.79 , max nb mot: 613 , min nb mot: 15 , esperance nb mot: 76.374 , moy nb phrase: 13.711 , max nb phrase: 35 , min nb phrase: 2 , esperance nb phrase: 4.546\n"
>>>>>>> 54953353516333b18a5c240985cdb19ed8b98b05
     ]
    }
   ],
   "source": [
    "lang = df[\"Lang\"].unique()\n",
    "res = {}\n",
    "for l in lang:\n",
    "    res[l] = {}\n",
    "\n",
    "for i in df[\"Lang\"].unique():\n",
    "    res[i][\"nb\"] = df[df[\"Lang\"] == i][\"Lang\"].count()\n",
    "    res[i][\"moy length\"] = df[df[\"Lang\"] == i][\"Text\"].str.len().mean()\n",
    "    res[i][\"max length\"] = df[df[\"Lang\"] == i][\"Text\"].str.len().max()\n",
    "    res[i][\"min length\"] = df[df[\"Lang\"] == i][\"Text\"].str.len().min()\n",
    "    res[i][\"esperance length\"] = df[df[\"Lang\"] == i][\"Text\"].str.len().std()\n",
    "    res[i][\"moy nb mot\"] = df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().mean()\n",
    "    res[i][\"max nb mot\"] = df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().max()\n",
    "    res[i][\"min nb mot\"] = df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().min()\n",
    "    res[i][\"esperance nb mot\"] = df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().std()\n",
    "    res[i][\"moy nb phrase\"] = df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().mean()\n",
    "    res[i][\"max nb phrase\"] = df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().max()\n",
    "    res[i][\"min nb phrase\"] = df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().min()\n",
    "    res[i][\"esperance nb phrase\"] = df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().std()\n",
    "    \n",
    "\n",
    "    print(  \n",
    "        \"lang =\", i, \", nb=\", round(df[df[\"Lang\"] == i][\"Lang\"].count(), 3), \n",
    "        \", moy length=\", round(df[df[\"Lang\"] == i][\"Text\"].str.len().mean(), 3), \n",
    "        \", max length=\", round(df[df[\"Lang\"] == i][\"Text\"].str.len().max(), 3),\n",
    "        \", min length=\", round(df[df[\"Lang\"] == i][\"Text\"].str.len().min(), 3),\n",
    "        \", esperance length=\", round(df[df[\"Lang\"] == i][\"Text\"].str.len().std(), 3),\n",
    "        \", moy nb mot :\", round(df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().mean(), 3),\n",
    "        \", max nb mot:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().max(), 3),\n",
    "        \", min nb mot:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().min(), 3),\n",
    "        \", esperance nb mot:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split().str.len().std(), 3),\n",
    "        \", moy nb phrase:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().mean(), 3),\n",
    "        \", max nb phrase:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().max(), 3),\n",
    "        \", min nb phrase:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().min(), 3),\n",
    "        \", esperance nb phrase:\", round(df[df[\"Lang\"] == i][\"Text\"].str.split(\".\").str.len().std(), 3),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = df[\"Text\"].str.len()\n",
    "df[\"mot\"] = df[\"Text\"].str.split().str.len()\n",
    "df[\"phrase\"] = df[\"Text\"].str.split(\".\").str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"Text\", \"mot\", \"length\", \"phrase\"]], df[[\"Lang\"]], test_size=0.4, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(X_train['Text'])\n",
    "X_other_numerical = X_train[[\"mot\", \"length\", \"phrase\"]].values\n",
    "X_train = hstack((X_text, X_other_numerical))\n",
    "\n",
    "X_text = vectorizer.fit_transform(X_test['Text'])\n",
    "X_other_numerical = X_test[[\"mot\", \"length\", \"phrase\"]].values\n",
    "X_test = hstack((X_text, X_other_numerical))\n",
    "\n",
    "X_all_text = vectorizer.fit_transform(df['Text'])\n",
    "X_other_numerical = df[[\"mot\", \"length\", \"phrase\"]].values\n",
    "X_all = hstack((X_all_text, X_other_numerical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9900x52223 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1477704 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
=======
>>>>>>> 54953353516333b18a5c240985cdb19ed8b98b05
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.592929292929293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fred/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, df[[\"Lang\"]], test_size=0.4, random_state=42)\n",
    "\n",
    "scaler = preprocessing.StandardScaler(with_mean=False).fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_scaled, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "# scaler = preprocessing.StandardScaler(with_mean=False).fit(X_test)\n",
    "y_pred = model.predict(scaler.transform(X_test))\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 7,
>>>>>>> 93864a4 (Dataset global shuffled)
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[\"Lang\"].unique():\n",
    "    df_ara = pd.read_csv(f\"data/{i}.csv\")\n",
    "    df_ara.sample(frac=1).reset_index(drop=True).to_csv(f\"data/{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ITA</td>\n",
       "      <td>I 'm convinced that having a broad knowledge o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRE</td>\n",
       "      <td>Do young people enjoy life more than older peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GER</td>\n",
       "      <td>There are many possibilities for young people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TUR</td>\n",
       "      <td>Transportation is a very importnat factor in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARA</td>\n",
       "      <td>There is a wide difference between an idea and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHI</td>\n",
       "      <td>Mobile industry has developed for decades and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JPN</td>\n",
       "      <td>Experiencing the convinience of the car , we c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HIN</td>\n",
       "      <td>It my belife that whole purpose of education i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPA</td>\n",
       "      <td>I agree that `` Most advertisements make produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KOR</td>\n",
       "      <td>Have you ever bought a product what you do not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TEL</td>\n",
       "      <td>knowledge is nothing but knowing things of unk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CHI</td>\n",
       "      <td>I agree with the saying that people should hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KOR</td>\n",
       "      <td>It is natural that a student studying harder c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HIN</td>\n",
       "      <td>An individual enjoys most of his life in his y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JPN</td>\n",
       "      <td>Some people say that young people nowadays giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ITA  I 'm convinced that having a broad knowledge o...\n",
       "1   FRE  Do young people enjoy life more than older peo...\n",
       "2   GER  There are many possibilities for young people ...\n",
       "3   TUR  Transportation is a very importnat factor in t...\n",
       "4   ARA  There is a wide difference between an idea and...\n",
       "5   CHI  Mobile industry has developed for decades and ...\n",
       "6   JPN  Experiencing the convinience of the car , we c...\n",
       "7   HIN  It my belife that whole purpose of education i...\n",
       "8   SPA  I agree that `` Most advertisements make produ...\n",
       "9   KOR  Have you ever bought a product what you do not...\n",
       "10  TEL  knowledge is nothing but knowing things of unk...\n",
       "11  CHI  I agree with the saying that people should hav...\n",
       "12  KOR  It is natural that a student studying harder c...\n",
       "13  HIN  An individual enjoys most of his life in his y...\n",
       "14  JPN  Some people say that young people nowadays giv..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "\n",
    "files = glob.glob(\"data/*.csv\")\n",
    "\n",
    "# Read lines from each file\n",
    "lines_from_each_file = []\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[1:]\n",
    "        lines_from_each_file.append(lines)\n",
    "\n",
    "# Alternate lines from each file with random order in each cycle\n",
    "combined_lines = []\n",
    "num_lines = len(lines_from_each_file[0])\n",
    "for i in range(num_lines):\n",
    "    # Shuffle the order of files for each line\n",
    "    random.shuffle(lines_from_each_file)\n",
    "\n",
    "    # Add one line from each file in the shuffled order\n",
    "    for lines in lines_from_each_file:\n",
    "        combined_lines.append(lines[i])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([line.split(r',\"') for line in combined_lines])\n",
    "df.drop(columns=[2], inplace=True)\n",
    "df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".extract_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
