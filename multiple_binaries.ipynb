{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import (get_data, display_results,\n",
    "                   get_label_encoder, get_train_dev_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lang</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GER</td>\n",
       "      <td>IThe importance and popularity of travelling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUR</td>\n",
       "      <td>It is an important decision , how to plan you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHI</td>\n",
       "      <td>Some people believe that young people can enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEL</td>\n",
       "      <td>Travelling is usually considered as good recr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARA</td>\n",
       "      <td>i agree that . Life is a person live period o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Lang                                               Text\n",
       "0  GER   IThe importance and popularity of travelling ...\n",
       "1  TUR   It is an important decision , how to plan you...\n",
       "2  CHI   Some people believe that young people can enj...\n",
       "3  TEL   Travelling is usually considered as good recr...\n",
       "4  ARA   i agree that . Life is a person live period o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ARA', 'CHI', 'FRE', 'GER', 'HIN', 'ITA', 'JPN', 'KOR', 'OTH',\n",
       "       'SPA', 'TEL', 'TUR'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.append(df[\"Lang\"], \"OTH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder, labels = get_label_encoder(np.unique(np.append(df[\"Lang\"], \"OTH\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARA': 0,\n",
       " 'CHI': 1,\n",
       " 'FRE': 2,\n",
       " 'GER': 3,\n",
       " 'HIN': 4,\n",
       " 'ITA': 5,\n",
       " 'JPN': 6,\n",
       " 'KOR': 7,\n",
       " 'OTH': 8,\n",
       " 'SPA': 9,\n",
       " 'TEL': 10,\n",
       " 'TUR': 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_split(df, language):\n",
    "    lang = df[df[\"Lang\"]==language]\n",
    "\n",
    "    other = df[df[\"Lang\"]!=language]\n",
    "    other = other.sample(len(lang))\n",
    "    other[\"Lang\"] = \"OTH\"\n",
    "\n",
    "    df_concat = pd.concat([lang, other])\n",
    "    df_concat = df_concat.sample(frac=1, random_state=42)\n",
    "\n",
    "    X, y = df_concat[\"Text\"], df_concat[\"Lang\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    true = 0\n",
    "    false = 0\n",
    "    total_size = 0\n",
    "    one_size = 0\n",
    "    size_zero = 0\n",
    "    true_size_one = 0\n",
    "\n",
    "    different = []\n",
    "    total_different = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i] not in different:\n",
    "            different.append(y_pred[i])\n",
    "            total_different += 1\n",
    "        if len(y_pred[i]) == 1:\n",
    "            if y_true[i] == y_pred[i][0]:\n",
    "                true_size_one += 1\n",
    "            one_size += 1\n",
    "        if len(y_pred[i]) == 0:\n",
    "            size_zero += 1\n",
    "        total_size += len(y_pred[i])\n",
    "        if y_true[i] in y_pred[i]:\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "    res = {\n",
    "        \"accuracy\": true/(false+true),\n",
    "        \"mean_size_tab\": total_size/len(y_true),\n",
    "        \"ratio_size_one\": one_size/len(y_true),\n",
    "        \"nb_different_tab\": total_different/len(y_true),\n",
    "        \"size_zero\": size_zero,\n",
    "        \"accuracy_size_one\": true_size_one/one_size\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_idx, test_idx):\n",
    "    df = get_data()\n",
    "    array = np.append(df[\"Lang\"], \"OTH\")\n",
    "    label_encoder, _ = get_label_encoder(array)\n",
    "    train, test = df.loc[train_idx], df.loc[test_idx]\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    # Train the models\n",
    "    for lang in df[\"Lang\"].unique():\n",
    "        clf = make_pipeline(\n",
    "            CountVectorizer(ngram_range=(1, 2)),\n",
    "            SGDClassifier()\n",
    "        )\n",
    "        X_train, y_train = get_df_split(train, lang)\n",
    "        X_dev, y_dev = get_df_split(test, lang)\n",
    "\n",
    "        y_train_labels = label_encoder.transform(y_train)\n",
    "        y_dev_labels = label_encoder.transform(y_dev)\n",
    "\n",
    "        clf.fit(X_train, y_train_labels)\n",
    "        \n",
    "        pred = clf.predict(X_dev)\n",
    "\n",
    "        models[lang] = {\"model\":clf, \"y_pred\":pred, \"y_true\":y_dev_labels}\n",
    "\n",
    "    results = {}\n",
    "    y_true = label_encoder.transform(test[\"Lang\"])\n",
    "    # Use the models\n",
    "    for model in models:\n",
    "        pred = models[model][\"model\"].predict(test[\"Text\"])\n",
    "        results[model] = {\"y_pred\":pred, \"y_true\":y_true}\n",
    "    \n",
    "    pred = []\n",
    "    for lang in results:\n",
    "        y_pred = results[lang][\"y_pred\"]\n",
    "        pred.append(y_pred)\n",
    "\n",
    "    real_pred = [[] for _ in range(len(pred[0]))]\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(len(pred[i])):\n",
    "            if pred[i][j] != 8:\n",
    "                real_pred[j].append(pred[i][j])\n",
    "\n",
    "    \n",
    "\n",
    "    return models, results, get_accuracy(y_true, real_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _BaseKFold.split at 0x717270a6d770>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m train_index\n\u001b[1;32m      7\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m test_index\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 31\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(train_idx, test_idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Use the models\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 31\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     results[model] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m:pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m:y_true}\n\u001b[1;32m     34\u001b[0m pred \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/repos/lang_detection/.lang_venv/lib/python3.10/site-packages/sklearn/pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/repos/lang_detection/.lang_venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/repos/lang_detection/.lang_venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Documents/repos/lang_detection/.lang_venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = df[\"Text\"], df[\"Lang\"]\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = skf.split(X, y)\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    train_idx = train_index\n",
    "    test_idx = test_index\n",
    "    _, res, acc = train_models(train_index, test_index)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sklearn(train_idx, test_idx):\n",
    "    train, test = df.loc[train_idx], df.loc[test_idx]\n",
    "\n",
    "    X_train, y_train = train[\"Text\"], train[\"Lang\"]\n",
    "    X_test, y_test = test[\"Text\"], test[\"Lang\"]\n",
    "\n",
    "    label_encoder, labels = get_label_encoder(np.unique(df[\"Lang\"]))\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    y_train = label_encoder.transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    base_lr = SGDClassifier(loss=\"log_loss\", learning_rate=\"adaptive\", eta0=0.1, max_iter=1000, tol=1e-3)\n",
    "    # base_lin = LogisticRegression(solver=\"liblinear\")\n",
    "    ovr = OneVsRestClassifier(base_lr)\n",
    "    ovr.fit(X_train, y_train)\n",
    "    Y_pred_ovr = ovr.predict(X_test)\n",
    "    print(accuracy_score(y_test, Y_pred_ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7126262626262626"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df[\"Text\"], df[\"Lang\"]\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = skf.split(X, y)\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    train_idx = train_index\n",
    "    test_idx = test_index\n",
    "    train_sklearn(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer()\n",
    "\n",
    "- 0.6904040404040404 -> LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "#### CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "- 0.7388888888888889 -> LogisticRegression(solver=\"liblinear\")\n",
    "- 0.7151515151515152 -> SGDClassifier()\n",
    "- 0.7136363636363636 -> SGDClassifier(loss=\"log_loss\")\n",
    "- 0.7136363636363636 -> SGDClassifier(loss=\"log_loss\", learning_rate=\"adaptive\", eta0=0.1, max_iter=1000, tol=1e-3)\n",
    "\n",
    "#### TfidfVectorizer()\n",
    "\n",
    "- 0.6853535353535354 -> SGDClassifier(loss=\"log_loss\", learning_rate=\"adaptive\", eta0=0.1, max_iter=1000, tol=1e-3)\n",
    "- 0.6737373737373737 -> LogisticRegression(solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test JOUR-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lang</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GER</td>\n",
       "      <td>I disagree with this statement because i thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GER</td>\n",
       "      <td>Yes , I do agree with the quoted statement . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GER</td>\n",
       "      <td>In my opinion young people do not support the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GER</td>\n",
       "      <td>Since I am attending the final year of second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GER</td>\n",
       "      <td>Since most of our neighbours are pretty old p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Lang                                               Text\n",
       "0  GER   I disagree with this statement because i thin...\n",
       "1  GER   Yes , I do agree with the quoted statement . ...\n",
       "2  GER   In my opinion young people do not support the...\n",
       "3  GER   Since I am attending the final year of second...\n",
       "4  GER   Since most of our neighbours are pretty old p..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"test.csv\"\n",
    "\n",
    "test = pd.read_csv(file_name)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"test.csv\"\n",
    "\n",
    "test = pd.read_csv(file_name)\n",
    "\n",
    "X_test = test[\"Text\"]\n",
    "\n",
    "df = get_data()\n",
    "\n",
    "X_train, y_train = df[\"Text\"], df[\"Lang\"]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "X_vect_test = vectorizer.transform(X_test)\n",
    "\n",
    "base_lr = SGDClassifier()\n",
    "ovr = OneVsRestClassifier(base_lr)\n",
    "ovr.fit(X_train, y_train)\n",
    "Y_pred_ovr = ovr.predict(X_vect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(Y_pred_ovr, columns=[\"Lang\"])\n",
    "df_res.to_csv(\"test_res.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
